{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS4 Coding #\n",
    "\n",
    "This assignment will have us looking to build a deep convolutional residual network neural network that also incorporates dropout. \n",
    "\n",
    "This network will use the softmax function to make a 10 class image classification on the MNIST data set (the original MNIST and not the fashion_mnist we've been working with thusfar)\n",
    "\n",
    "\n",
    "But before you get started, please make sure you have the following packages installed\n",
    "## Packages to install:\n",
    "1. numpy\n",
    "2. keras\n",
    "3. tensorflow\n",
    "4. matplotlib\n",
    "\n",
    "For keras and tensorflow, please refer to this link (https://docs.floydhub.com/guides/environments/) to make sure you install versions that are compatible with each other. I would highly recommend getting tensorflow==1.14.1 and the compatible keras version. The exact python version, as long as it's python3+, should not impact your ability to use these two packages.\n",
    "\n",
    "## Structure of Assigment ##\n",
    "\n",
    "What's new compared to PS4:\n",
    "1. **Skip Layer Residual Connections**\n",
    "2. **Drop out**\n",
    "\n",
    "\n",
    "## Terminology\n",
    "Please look over the power point under Piazza > Resources > ConvolutionalNetwork.ppt to make sure you understand exactly what I mean when I type the following terms:\n",
    "1. Applying a kernel/filter\n",
    "2. Kernel/Filter\n",
    "3. Max Pool\n",
    "4. Feature map\n",
    "5. Convolution\n",
    "\n",
    "## Network Architecture\n",
    "![title](./ps5_img/ResNet1.png)\n",
    "\n",
    "It is quite similar to PS4, and in fact I recommend that the first step you take is to:\n",
    "\n",
    "## copy your code from PS4 in the cells below\n",
    "\n",
    "\n",
    "Remember the following equation: **out_shape is an integer greater than 1**, where $$out\\_shape = \\frac{input\\_shape + 2 * padding - kernel\\_shape}{stride} + 1$$\n",
    "\n",
    "\n",
    "## Assignment Grading and Procedure Recommendation ##\n",
    "1. If you correctly implement the residual skip layer connections with a single kernel at each conv layer, you will recieve 75%\n",
    "2. Correct implementation of drop out will recieve 20%\n",
    "3. Experimentation section will give the last 5%\n",
    "\n",
    "Here is how I recommend going about this assignment:\n",
    "1. Copy over your code from PS4\n",
    "2. Modify code so that it has the residual connections\n",
    "3. Implement dropout feature\n",
    "4. Experiment with parameter tuning of your convolutional residual network!\n",
    "\n",
    "### A note on \"different kernel shape\": \n",
    "For a convolutional network, all kernels applied at the same layer will be the **same shape**, when I say that your network should be able to handle differnt kernel shapes, that means if you change the kernel shape at a given layer, **all kernels applied on that layer will adopt the new shape**.\n",
    "\n",
    "For instance, Kernel1 begins as a 4x4 kernel. This means if I wanted to apply multiple Kernel1's to the input layer, then I will apply multiple **4x4 kernels** (they are all the same shape). If I change my network such that Kernel1 is now a **6x6 kernel** that means ALL applications of kernel1 will now be 6x6.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "You will notice that this assignment has very little headers and comments. I am leaving it up to you to decide exactly what info you need to incorporate for each function as a parameter, and the functionality and output of each that function. Feel free to use the previous problem sets as models for how to model your code. I recommend you continue to format your data in terms of N x M\n",
    "\n",
    "1. N = number of features\n",
    "2. M = number of data points\n",
    "\n",
    "\n",
    "## Residual Skip Layer connections\n",
    "\n",
    "Your model will ahve two skip layer connections, the first from the output of maxpool_1 with the output of conv_3, and the second with the output of conv_3 and the output of conv_5.\n",
    "\n",
    "Remember that at each residual connection, you simply add the two outputs together. For instance, at the first residual connection, the output of conv_3 would become: $$Conv3 = Conv3 + MaxPool1$$.\n",
    "\n",
    "At the second residual connection, we would get the following equation: $$Conv5 = Conv5 + (Conv3 + MaxPool1)$$\n",
    "\n",
    "Your task is to implement these skip layer connections. \n",
    "\n",
    "Remember that the skip layer connection itself is not learnable, it is simply an identity mapping. \n",
    "\n",
    "## Dropout\n",
    "\n",
    "The core idea behind drop out is quite simple: for each node in the network, flip a weighted coin. If the coin returns heads, keep the node in the network, else \"drop\" it during training.\n",
    "\n",
    "Practically, this looks like the following: After the final computation of a layers output (including AFTER the residual addition), compute a binary vector, p, in N-space, where N is the number of nodes in that layer. The assignment of p should be based on a uniform probability distribution, with $\\mu$ chance of $x_i = 1$, and $1-\\mu$ chance of $x_i = 0$. You then element wise multiply this binary vector to the output of that respective layer. \n",
    "\n",
    "The binary mask should be generated randomly for every layer, during each epoch, and for each batch of data. \n",
    "\n",
    "This binary mask will also be used in back propogation, similar to the binary mask from max-pooling, but note that you should NOT apply dropout to max-pool layers. \n",
    "\n",
    "Your task is to incorporate dropout into your convolutional residual network. \n",
    "\n",
    "## Data management\n",
    "\n",
    "We will be using four data sets for this problem set. \n",
    "1. MNIST (the most popular computer vision data set)\n",
    "2. Dummy data (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and BackProp functions\n",
    "\n",
    "![title](./ps5_img/backprop.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Left here for a copy of your PS4 code\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "def gen_dummy():\n",
    "    '''\n",
    "    dummy data is exceptionally useful to test whether or not your network behaves as expected.\n",
    "    For dummy data, you should generate a few (<= 5) input/output pairs that you can use to test\n",
    "    your forward and backward propagation algorithms\n",
    "    \n",
    "    output: \n",
    "    dummy_x = a NxM np matrix, both dimensions of your choosing of very simple data\n",
    "    dummy_y = a (M, ) np array with the corresponding labels\n",
    "    '''\n",
    "    dummy_x = []\n",
    "    dummy_y = []\n",
    "    ####BEGIN CODE HERE####\n",
    "    \n",
    "    N = 28 * 28\n",
    "    M = 5\n",
    "    \n",
    "    dummy_x = np.ones((N, M))\n",
    "    dummy_y = np.ones(M)\n",
    "    ####END CODE HERE####\n",
    "\n",
    "    return dummy_x, dummy_y\n",
    "\n",
    "def load_mnist():\n",
    "    ''' \n",
    "    look up how to load the mnist data set via keras\n",
    "    '''\n",
    "    mnist = keras.datasets.mnist\n",
    "    (mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()\n",
    "    return mnist_train_images, mnist_train_labels, mnist_test_images, mnist_test_labels\n",
    "\n",
    "\n",
    "def flatten_normalize(images):\n",
    "    '''\n",
    "    convert the image from a N1xN1xM to NxM format where N1 = square_root(N) and normalize\n",
    "    '''\n",
    "    images = np.reshape(images, (-1, images.shape[-1]))\n",
    "    images = np.divide(images, np.max(images, 1))\n",
    "    return images\n",
    "\n",
    "\n",
    "def subset_mnist_training():\n",
    "    '''\n",
    "    Return 100 training samples from each of the 10 classes, 1000 samples all together\n",
    "    '''\n",
    "    mnist_train_images, mnist_train_labels, mnist_test_images, mnist_test_labels = load_mnist()\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in np.unique(mnist_train_labels):\n",
    "        train = mnist_train_images[mnist_train_labels == i]\n",
    "        train = train[np.random.choice(train.shape[0], 100, False)]\n",
    "        images.extend(train)\n",
    "        labels.extend([i for k in range(100)])\n",
    "        \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def subset_mnist_testing():\n",
    "    '''\n",
    "    Return 20 training samples from each of the 10 classes, 200 samples all together\n",
    "    '''\n",
    "    mnist_train_images, mnist_train_labels, mnist_test_images, mnist_test_labels = load_mnist()\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in np.unique(mnist_test_labels):\n",
    "        test = mnist_test_images[mnist_test_labels == i]\n",
    "        test = test[np.random.choice(test.shape[0], 20, False)]\n",
    "        imgs.extend(test)\n",
    "        labels.extend([i for k in range(20)])\n",
    "        \n",
    "    return images, labels\n",
    "\n",
    "def load_data():\n",
    "    ''' \n",
    "    look up how to load the mnist data set via keras\n",
    "    '''\n",
    "    mnist = keras.datasets.mnist\n",
    "    (tx, ty), (ex, ey) = mnist.load_data()\n",
    "    return (tx/255, ty), (ex/255, ey)\n",
    "    \n",
    "def kernel_initialization(w1, w2):\n",
    "    '''\n",
    "    returns a kernel with specified height and width. \n",
    "    The values of the kernel should be initialized using the same formula as the\n",
    "    He_initialize_weight() function\n",
    "    '''\n",
    "    ret = np.random.rand(w1, w2) * np.sqrt(2 / max(w1, w2))\n",
    "    return ret\n",
    "\n",
    "def He_initialize_weight(w1, w2):\n",
    "    '''\n",
    "    (same as PS3)\n",
    "    returns a weight matrix with the passed in dimensions\n",
    "    '''\n",
    "    ret = np.random.rand(w1, w2) * np.sqrt(2 / max(w1, w2))\n",
    "    return ret\n",
    "\n",
    "def bias_initialization(w1):\n",
    "    ''' (same as PS3)\n",
    "    returns a bias matrix of the passed in dimensions\n",
    "    '''\n",
    "    ret = np.random.rand(w1) * np.sqrt(2 / w1)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def log_cost(label, prediction):\n",
    "    '''\n",
    "    computes the log cost of the current predictions using the labels (same as PS3)\n",
    "    '''\n",
    "    cost = 0\n",
    "    for i in range(label.shape[0]):\n",
    "        cost += -np.log(prediction[int(label[i]), i])\n",
    "    return cost\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    computes the softmax of the input (same as PS3)\n",
    "    '''\n",
    "    ret = np.zeros(x.shape)\n",
    "    for col in range(x.shape[1]):\n",
    "        total = np.sum(np.exp(x[:, col]))\n",
    "        ret[:, col] = np.exp(x[:, col]) / total\n",
    "    return ret\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    '''\n",
    "    computes the ReLU of the input (same as PS3)\n",
    "    '''\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    ''''\n",
    "    computes the ReLU' of the input (same as PS3)\n",
    "    '''\n",
    "    ret = np.copy(x)\n",
    "    ret[ret > 0] = 1\n",
    "    ret[ret <= 0] = 0\n",
    "    return ret\n",
    "\n",
    "\n",
    "def kernel_to_matrix(kernal: np.ndarray, input_size):\n",
    "    '''\n",
    "    converts a kernel to its matrix form\n",
    "    '''\n",
    "    kernal_size = kernal.shape[0]\n",
    "    output_size = input_size - kernal_size + 1\n",
    "\n",
    "    kernal = kernal.flatten()\n",
    "    to_right = input_size - kernal_size\n",
    "    ret = np.zeros((output_size * output_size, input_size * input_size))\n",
    "    t0 = 0\n",
    "    for i in range(output_size * output_size):\n",
    "        t1 = i\n",
    "        for j in range(kernal.shape[0]):\n",
    "            ret[i, j + t0 + t1] = kernal[j]\n",
    "            if (j + 1) % kernal_size == 0:\n",
    "                t1 += to_right\n",
    "        if (i + 1) % output_size == 0:\n",
    "            t0 += (kernal_size - 1)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def max_pool(X):\n",
    "    '''\n",
    "    applies max-pooling to an input image\n",
    "    '''\n",
    "    size = int(np.sqrt(X.shape[0]))\n",
    "    out_size = size // 2\n",
    "    ret = np.empty((out_size, out_size, X.shape[-2], X.shape[-1]))\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(X.shape[2]):\n",
    "            t1 = np.reshape(X[:, i, j], (size, size))\n",
    "            for x in range(out_size):\n",
    "                for y in range(out_size):\n",
    "                    ret[x, y, i, j] = np.max(t1[x * 2: (x + 1) * 2, y * 2: (y + 1) * 2])\n",
    "    return np.reshape(ret, (out_size * out_size, X.shape[-2], X.shape[-1]))\n",
    "\n",
    "\n",
    "def max_pool_backwards(delta_el, z):\n",
    "    '''\n",
    "    takes the output of a maxpool, and projects back to the original shape.\n",
    "    see PPT slides on convolutional backprop if you have no idea what I'm talking about.\n",
    "    '''\n",
    "    small_size = int(np.sqrt(delta_el.shape[0]))\n",
    "    big_size = small_size * 2 + 1\n",
    "    origin = np.reshape(delta_el, (small_size, small_size, delta_el.shape[-2], delta_el.shape[-1]))\n",
    "    z = np.reshape(z, (big_size, big_size, z.shape[-2], z.shape[-1]))\n",
    "    ret = np.zeros((big_size, big_size, delta_el.shape[-2], delta_el.shape[-1]))\n",
    "    for b in range(origin.shape[-2]):\n",
    "        for layer in range(origin.shape[-1]):\n",
    "            for i in range(small_size):\n",
    "                for j in range(small_size):\n",
    "                    index = np.unravel_index(np.argmax(z[i * 2: (i + 1) * 2, j * 2: (j + 1) * 2, b, layer].flatten()), (2, 2))\n",
    "                    ret[i * 2 + index[0], j * 2 + index[1], b, layer] = origin[i, j, b, layer]\n",
    "    return ret.reshape((-1, ret.shape[-2], ret.shape[-1]))\n",
    "\n",
    "\n",
    "def delta_Last(predictions, labels):\n",
    "    '''\n",
    "    task: computer error term for ONLY output layer\n",
    "    '''\n",
    "    return predictions - labels\n",
    "\n",
    "\n",
    "def delta_el(weight_d_plus_1, delta_d_plus_1, input_d):\n",
    "    '''\n",
    "    task: compute error term for any hidden layer\n",
    "    '''\n",
    "    ####BEGIN CODE HERE ####\n",
    "    return np.matmul(weight_d_plus_1, delta_d_plus_1) * ReLU_prime(input_d)\n",
    "    ####END CODE HERE####\n",
    "\n",
    "\n",
    "def dW(delta_d, a_d_minus_1):\n",
    "    '''\n",
    "    task: compute gradient for any weight matrix\n",
    "    '''\n",
    "    return np.matmul(delta_d, a_d_minus_1.T)\n",
    "\n",
    "\n",
    "def db(delta_d):\n",
    "    '''\n",
    "    task: compute gradient for any bias term\n",
    "    '''\n",
    "    return np.sum(delta_d, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def weight_upate(weights, gradients, learning_rate):\n",
    "    '''\n",
    "\n",
    "    task: udpate each of the weight matrices, and return them in a variable, name of  your choosing\n",
    "    '''\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= learning_rate * gradients[i]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def bias_update(bias, bias_grad, learning_rate):\n",
    "    '''\n",
    "    task : update each of the bias terms, return them in a variable, name of  your choosing\n",
    "    '''\n",
    "    for i in range(len(bias)):\n",
    "        bias[i] -= learning_rate * bias_grad[i]\n",
    "    return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, weights, bias, activations, dropout):\n",
    "    '''\n",
    "    minimum output: the predictions made by the network\n",
    "    \n",
    "    You are free to return more things from this function if you see fit\n",
    "    \n",
    "    hint: you will need to return all intermediate computations, not just the output\n",
    "    To figure out what you need to return, look at what intermediate results you need\n",
    "    to compute backpropagation\n",
    "    \n",
    "    you can return more than one variable with the following syntax:\n",
    "    \n",
    "    return var1, var2, ..., varN\n",
    "    '''\n",
    "    out = x\n",
    "    z = [x]\n",
    "    a = [x]\n",
    "    for i in range(len(activations)):\n",
    "        if activations[i] == \"max\" and isinstance(weights[i], list):\n",
    "            if len(out.shape) <= 2:\n",
    "                out = out[..., np.newaxis]\n",
    "            layer_num = 1 if len(out.shape) <= 2 else out.shape[2]\n",
    "            t = np.empty((weights[i][0].shape[0], out.shape[1], len(weights[i]) * layer_num))\n",
    "            for j in range(len(weights[i])):\n",
    "                for k in range(layer_num):\n",
    "                    t[:, :, j * layer_num + k] = np.matmul(weights[i][j], out[:, :, k])\n",
    "            out = t\n",
    "            if activations[i + 1] == 'relu':\n",
    "                z.append(np.transpose(out, (0, 2, 1)).reshape((-1, out.shape[-2])))\n",
    "            else:\n",
    "                z.append(out)\n",
    "            out = max_pool(out)\n",
    "            a.append(out)\n",
    "        elif activations[i] == 'relu':\n",
    "            if len(out.shape) > 2:\n",
    "                out = np.transpose(out, (0, 2, 1))\n",
    "                out = out.reshape((-1, out.shape[-1]))\n",
    "                a[-1] = out\n",
    "            out = np.add(np.matmul(weights[i].T, out), bias[i][..., np.newaxis])\n",
    "            z.append(out)\n",
    "            out = ReLU(out)\n",
    "            a.append(out)\n",
    "        elif activations[i] == \"softmax\":\n",
    "            out = softmax(out)\n",
    "            a.append(out)\n",
    "        elif activations[i] == \"dropout\":\n",
    "            out = dropout_forward(out, dropout)\n",
    "            a.append(out)\n",
    "    \n",
    "    return out, z, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, weights, bias, activations, layers, epoch, lr, dropout):\n",
    "    \n",
    "    num_samples = x.shape[1]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print('--------epoch', i, \"--------------\")\n",
    "        out, z, a = predict(x, weights, bias, activations, dropout)\n",
    "\n",
    "        cost = log_cost(y, out)\n",
    "        print(\"epoch: \", i, \" cost: \", cost)\n",
    "        # backward\n",
    "        delta_l = delta_Last(out, y)\n",
    "        \n",
    "        d_W = [np.zeros_like(w) for w in weights]\n",
    "        d_b = [np.zeros_like(w) for w in bias]\n",
    "        for j in range(2, len(activations) - 2):\n",
    "            if activations[-j] == \"relu\":\n",
    "                d_W_t = dW(delta_l, a[-j - 1]).T / num_samples\n",
    "                d_b_t = db(delta_l) / num_samples\n",
    "                d_W.append(d_W_t)\n",
    "                d_b.append(d_b_t)\n",
    "                if activations[-j - 1] != \"relu\":\n",
    "                    delta_l = delta_el(weights[-j + 1], delta_l, a[-j - 1])\n",
    "                else:\n",
    "                    delta_l = delta_el(weights[-j + 1], delta_l, z[-j - 1])\n",
    "            elif activations[-j] == \"max\":\n",
    "                layer = 1\n",
    "                pre_z = z[-j + 1]\n",
    "                for k in range(len(weights) - j + 2):\n",
    "                    layer = layer * len(weights[k])\n",
    "                if activations[-j + 1] != \"max\":\n",
    "                    delta_l = delta_l.reshape((-1, layer, delta_l.shape[-1]))\n",
    "                    pre_z = pre_z.reshape((-1, layer, delta_l.shape[-1]))\n",
    "                delta_l = max_pool_backwards(delta_l, pre_z)\n",
    "                if layers[-j + 1] == \"residual\":\n",
    "                    delta_l = delta_l.reshape((-1, layer, delta_l.shape[-1]))\n",
    "                    pre_z = pre_z.reshape((-1, layer, delta_l.shape[-1]))\n",
    "                delta_l = max_pool_backwards(delta_l, pre_z)\n",
    "                d_W_t = []\n",
    "                d_b_t = []\n",
    "                for k in range(layer):\n",
    "                    d_W_t.append(dW(delta_l[:, k, :], a[-j - 1][:, :, k // len(weights[-j + 1])]).T / num_samples)\n",
    "                    d_b_t.append(db(delta_l[:, k, :]) / num_samples)\n",
    "                d_W.append(d_W_t)\n",
    "                d_b.append(d_b_t)\n",
    "                if j == len(activations):\n",
    "                    continue\n",
    "                t_delta_l = []\n",
    "                for k in range(layer // len(weights[-j + 1])):\n",
    "                    t_delta_l.append(delta_el(weights[-j][k], delta_l[:, k * len(weights[-j+1]), :], z[-j - 1]))\n",
    "                    t_delta_l.append(delta_el(weights[-j][k], delta_l[:, k * len(weights[-j+1]) + 1, :], z[-j - 1]))\n",
    "            elif activations[-j] == \"dropout\":\n",
    "                layer = 1\n",
    "                pre_z = z[-j + 1]\n",
    "                for k in range(len(weights) - j + 2):\n",
    "                    layer = layer * len(weights[k])\n",
    "                if activations[-j + 1] != \"dropout\":\n",
    "                    delta_l = delta_l.reshape((-1, layer, delta_l.shape[-1]))\n",
    "                    pre_z = pre_z.reshape((-1, layer, delta_l.shape[-1]))\n",
    "                delta_l = max_pool_backwards(delta_l, pre_z)\n",
    "                d_W_t = []\n",
    "                d_b_t = []\n",
    "            else:\n",
    "                raise Exception()\n",
    "            weights = weight_upate(weights, d_W, lr)\n",
    "            bias = bias_update(bias, d_b, lr)\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual skip layer\n",
    "def residual_forward(x1, x2):\n",
    "    return x1+x2\n",
    "\n",
    "\n",
    "def residual_backward(g, x1, x2):\n",
    "    s = x1+x2\n",
    "    return g*(x1/s), g*(x2/s)\n",
    "\n",
    "\n",
    "# dropout layer\n",
    "def dropout_forward(x, prob):\n",
    "    \"\"\"\n",
    "    :param x x.shape should be [n,n,m]\n",
    "    \"\"\"\n",
    "    mask = (np.random.rand(*x.shape) < self._keep_prob)\n",
    "    x *= mask\n",
    "    x /= prob\n",
    "    return x, mask\n",
    "\n",
    "\n",
    "def dropout_forward(g, mask):\n",
    "    g *= mask\n",
    "    g /= prob\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------epoch 0 --------------\n",
      "epoch:  0  cost:  108.8673757619556\n",
      "--------epoch 1 --------------\n",
      "epoch:  1  cost:  108.8673757619556\n",
      "--------epoch 2 --------------\n",
      "epoch:  2  cost:  108.8673757619556\n",
      "--------epoch 3 --------------\n",
      "epoch:  3  cost:  108.8673757619556\n",
      "--------epoch 4 --------------\n",
      "epoch:  4  cost:  108.8673757619556\n",
      "--------epoch 5 --------------\n",
      "epoch:  5  cost:  108.8673757619556\n",
      "--------epoch 6 --------------\n",
      "epoch:  6  cost:  108.8673757619556\n",
      "--------epoch 7 --------------\n",
      "epoch:  7  cost:  108.8673757619556\n",
      "--------epoch 8 --------------\n",
      "epoch:  8  cost:  108.8673757619556\n",
      "--------epoch 9 --------------\n",
      "epoch:  9  cost:  108.8673757619556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[array([[0.51395805, 0.34728995, 0.59089185, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.51395805, 0.34728995, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.51395805, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.27927116, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.22538358, 0.27927116,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.08118517, 0.22538358,\n",
       "           0.27927116]]),\n",
       "   array([[0.1793265 , 0.4533772 , 0.05489483, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.1793265 , 0.4533772 , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.1793265 , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.04564939, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.10252226, 0.04564939,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.399251  , 0.10252226,\n",
       "           0.04564939]])],\n",
       "  [array([[0.31230327, 0.17904596, 0.13151854, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.31230327, 0.17904596, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.31230327, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.37500749, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.25051114, 0.37500749,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.22846635, 0.25051114,\n",
       "           0.37500749]]),\n",
       "   array([[0.67947007, 0.50960753, 0.66088692, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.67947007, 0.50960753, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.67947007, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.41008659, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.41543784, 0.41008659,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.40045964, 0.41543784,\n",
       "           0.41008659]])],\n",
       "  array([[1.36449462e-01, 5.76375084e-03, 9.22517877e-02, 1.14059495e-01,\n",
       "          1.27287675e-01, 7.65516290e-02, 2.47290131e-02, 1.65114547e-02,\n",
       "          5.93791021e-02, 1.17574245e-01],\n",
       "         [6.86761674e-02, 8.19606246e-02, 1.06527538e-01, 1.46217607e-01,\n",
       "          1.47835728e-02, 1.02282953e-02, 3.04476397e-02, 4.06521040e-02,\n",
       "          6.90243185e-02, 1.36765679e-01],\n",
       "         [8.77971191e-02, 1.03193088e-01, 1.71410182e-01, 1.57937547e-01,\n",
       "          1.69098233e-02, 1.56207216e-01, 4.01550754e-02, 9.33459836e-02,\n",
       "          8.25839567e-02, 1.79629795e-03],\n",
       "         [1.07936846e-01, 1.10819215e-01, 5.70388833e-03, 7.48913603e-02,\n",
       "          1.62787615e-02, 4.30290539e-02, 1.61600972e-01, 8.66850018e-02,\n",
       "          4.71814153e-02, 6.35574370e-02],\n",
       "         [1.67065745e-01, 1.76479402e-02, 1.76123276e-01, 6.39927237e-02,\n",
       "          9.88014149e-02, 5.95954313e-02, 1.92154155e-02, 5.94221421e-02,\n",
       "          1.04928435e-01, 7.59643028e-02],\n",
       "         [1.35520585e-01, 6.10853135e-02, 1.29251249e-01, 2.25040714e-02,\n",
       "          1.42387087e-01, 1.30618893e-01, 1.59848516e-01, 3.47282578e-02,\n",
       "          3.75893654e-02, 1.92430141e-02],\n",
       "         [1.01119219e-02, 1.16582273e-01, 1.42671046e-01, 1.50031591e-01,\n",
       "          7.91913220e-02, 1.12701391e-01, 9.56448751e-02, 1.14703788e-01,\n",
       "          1.70532085e-01, 4.42461336e-03],\n",
       "         [1.02259214e-04, 1.52617490e-01, 1.22147900e-01, 1.51317337e-01,\n",
       "          2.78063635e-02, 1.49509132e-01, 1.26604278e-01, 3.00777334e-02,\n",
       "          8.78945914e-03, 1.07221047e-01],\n",
       "         [4.40910244e-02, 1.60920576e-01, 1.04512750e-01, 1.20995263e-01,\n",
       "          2.86304640e-02, 1.05950256e-01, 7.68207807e-02, 2.05928265e-03,\n",
       "          9.50289350e-02, 1.63315914e-02],\n",
       "         [7.03394664e-03, 9.52085993e-02, 1.42679700e-01, 7.37224656e-02,\n",
       "          1.17538668e-01, 1.54034383e-01, 9.57052483e-02, 2.44515962e-02,\n",
       "          1.74872899e-02, 1.14671772e-01],\n",
       "         [7.72205923e-02, 1.90967365e-02, 3.23926788e-02, 2.69071622e-03,\n",
       "          4.97110302e-02, 1.04086380e-01, 1.63139482e-01, 5.47925009e-02,\n",
       "          1.01970064e-01, 1.28708617e-02],\n",
       "         [1.08138709e-01, 4.23446271e-02, 6.14846870e-02, 6.87749675e-02,\n",
       "          1.76149746e-01, 1.03758625e-01, 9.99818150e-02, 1.74708178e-01,\n",
       "          1.54069386e-01, 3.82924803e-02],\n",
       "         [1.60279979e-01, 1.53310649e-01, 1.07611852e-01, 3.93171353e-02,\n",
       "          1.45144301e-01, 1.28885988e-01, 1.58241534e-01, 1.38358692e-01,\n",
       "          3.27804468e-02, 7.21454158e-02],\n",
       "         [1.15539161e-01, 1.41440147e-01, 1.08050416e-01, 1.30031084e-01,\n",
       "          2.46835907e-02, 1.69480548e-01, 2.60314395e-02, 8.54384116e-02,\n",
       "          2.92839498e-02, 2.22657562e-02],\n",
       "         [6.52830663e-02, 3.20964338e-02, 1.53394935e-01, 3.92233650e-02,\n",
       "          1.07163742e-01, 2.08498715e-02, 1.48201420e-01, 7.05341030e-02,\n",
       "          1.36008775e-01, 5.04958095e-02],\n",
       "         [8.65062525e-02, 5.76684123e-03, 9.26181114e-02, 1.38069320e-01,\n",
       "          1.05229948e-01, 2.94743408e-03, 3.79975802e-02, 6.43744227e-02,\n",
       "          1.63376305e-01, 1.15579506e-02],\n",
       "         [8.94989685e-02, 1.62829657e-01, 1.29067395e-01, 1.44696113e-01,\n",
       "          1.56314560e-01, 5.09762966e-02, 1.03612312e-01, 5.70891115e-02,\n",
       "          7.34791723e-02, 1.63869372e-01],\n",
       "         [1.12852865e-02, 6.65788314e-02, 8.75792915e-02, 1.21435276e-01,\n",
       "          4.12412832e-02, 1.03846032e-02, 1.30775421e-01, 1.20274707e-01,\n",
       "          1.24972619e-02, 1.26464482e-01],\n",
       "         [1.60313650e-01, 2.85743799e-03, 5.01206244e-02, 1.48696795e-01,\n",
       "          1.54424146e-01, 5.02529762e-02, 2.43032815e-02, 1.73166617e-01,\n",
       "          4.15338276e-02, 1.29775095e-01],\n",
       "         [1.35201607e-01, 1.75791252e-01, 5.02968070e-02, 5.11619512e-02,\n",
       "          1.15115476e-01, 1.36510607e-01, 1.46938238e-01, 1.32343424e-02,\n",
       "          1.45078546e-01, 8.19488133e-02],\n",
       "         [6.02029606e-02, 1.33749086e-01, 1.48356104e-01, 5.42036130e-02,\n",
       "          1.23969884e-02, 3.00199195e-02, 8.64518857e-02, 1.11905703e-01,\n",
       "          8.70727031e-02, 1.43578270e-01],\n",
       "         [6.22817872e-02, 1.49599677e-01, 2.53279765e-03, 1.70029813e-01,\n",
       "          4.69580936e-02, 1.26999073e-01, 1.14895292e-02, 1.61972412e-01,\n",
       "          4.23752429e-02, 1.24845624e-01],\n",
       "         [4.23755829e-02, 1.50831635e-01, 1.57343768e-02, 1.61835069e-01,\n",
       "          1.21624356e-01, 5.72487460e-02, 6.33559266e-02, 1.63253372e-01,\n",
       "          1.40803209e-01, 1.40382565e-01],\n",
       "         [2.34378801e-02, 5.98862951e-02, 8.77798677e-02, 1.48988110e-01,\n",
       "          4.72951845e-02, 1.55738042e-01, 6.86519952e-02, 1.21581607e-01,\n",
       "          1.14296989e-01, 1.40366311e-01],\n",
       "         [1.57739826e-01, 4.99348885e-03, 8.20793588e-02, 1.74741673e-02,\n",
       "          8.65757620e-03, 1.81014717e-02, 1.41117693e-01, 1.48946681e-01,\n",
       "          1.07147026e-01, 4.07654168e-02],\n",
       "         [1.56408059e-01, 4.28657814e-02, 3.41994332e-03, 6.38853220e-02,\n",
       "          1.35757055e-01, 6.25252564e-03, 1.05123356e-01, 1.20207218e-01,\n",
       "          2.47649556e-03, 1.94303461e-02],\n",
       "         [1.02412758e-01, 5.33836584e-02, 1.31052865e-01, 2.00373674e-03,\n",
       "          1.64782022e-01, 5.82970225e-02, 1.36362540e-01, 1.66129923e-01,\n",
       "          1.37871343e-01, 9.01847872e-02],\n",
       "         [1.06169219e-01, 6.69445916e-02, 1.22522340e-01, 1.84280589e-02,\n",
       "          1.28152061e-01, 1.15416078e-01, 1.14167040e-02, 1.16565322e-01,\n",
       "          2.55270590e-02, 3.96531631e-02],\n",
       "         [3.84731114e-03, 1.00021078e-02, 2.13575914e-02, 7.84485840e-02,\n",
       "          2.64130969e-02, 1.31397110e-01, 1.99624044e-02, 1.22844815e-01,\n",
       "          1.57802845e-01, 1.16769593e-01],\n",
       "         [7.64050744e-02, 1.37591524e-01, 2.51086297e-02, 7.41454898e-02,\n",
       "          1.29973308e-01, 2.29963344e-04, 2.13472949e-02, 3.15885303e-03,\n",
       "          9.66316097e-02, 1.04252983e-01],\n",
       "         [6.74671827e-02, 2.36287979e-03, 8.18753149e-02, 1.54218864e-01,\n",
       "          7.62406813e-02, 3.90477676e-02, 1.46702650e-01, 1.55188038e-01,\n",
       "          6.26268771e-02, 1.55991527e-01],\n",
       "         [7.92391493e-02, 1.08928211e-01, 5.01174854e-02, 1.65118680e-01,\n",
       "          7.60116554e-02, 2.36686880e-02, 1.70895778e-01, 1.28496270e-01,\n",
       "          6.91806423e-02, 6.15028291e-03],\n",
       "         [1.43637989e-01, 1.19066215e-02, 1.29420919e-01, 1.33976827e-01,\n",
       "          4.10228331e-02, 4.20675769e-03, 9.14105383e-02, 7.95354267e-02,\n",
       "          1.29305862e-01, 1.66069046e-02],\n",
       "         [8.44542643e-04, 7.81161903e-02, 1.82331173e-02, 1.32010913e-01,\n",
       "          7.86924154e-02, 7.65815401e-02, 2.06472124e-02, 1.15884134e-01,\n",
       "          8.74691793e-02, 1.70994610e-01],\n",
       "         [7.13851049e-02, 1.32294263e-01, 7.00862475e-03, 8.41482171e-02,\n",
       "          7.44121918e-02, 6.09081466e-02, 1.55471851e-01, 1.73489799e-02,\n",
       "          6.41630062e-03, 4.67623546e-02],\n",
       "         [1.65749650e-01, 1.53462472e-01, 3.65025731e-02, 1.74376648e-01,\n",
       "          1.18662462e-01, 1.49660619e-01, 1.71836911e-01, 1.29802018e-01,\n",
       "          1.22663801e-01, 1.21576484e-01],\n",
       "         [1.17715876e-01, 5.21072369e-02, 1.59654530e-01, 1.24621644e-01,\n",
       "          1.67326174e-01, 1.44801668e-01, 2.65787217e-02, 2.02883647e-02,\n",
       "          1.31405488e-01, 1.49641807e-01],\n",
       "         [9.32633572e-02, 1.33670876e-01, 1.39279301e-01, 9.10976762e-02,\n",
       "          1.03933623e-01, 5.72294919e-02, 1.51366386e-01, 4.88276331e-02,\n",
       "          1.48574041e-01, 1.59197240e-01],\n",
       "         [3.31875068e-02, 4.62400938e-02, 4.96687105e-03, 9.66804246e-02,\n",
       "          1.75191751e-01, 4.58481488e-02, 1.10614241e-02, 5.30535588e-02,\n",
       "          1.47063623e-01, 1.54902596e-01],\n",
       "         [8.52365485e-02, 1.44363060e-01, 7.88297389e-02, 6.29664435e-02,\n",
       "          1.34787554e-01, 1.26435700e-02, 7.93415589e-02, 1.08434050e-01,\n",
       "          8.28261431e-02, 1.12747917e-01],\n",
       "         [1.41267221e-01, 1.06470815e-03, 8.80354472e-02, 1.70536077e-01,\n",
       "          8.91313397e-02, 1.20675246e-01, 1.25797647e-01, 1.55796811e-01,\n",
       "          3.02762667e-02, 1.37306022e-04],\n",
       "         [7.51475464e-02, 1.23518038e-01, 3.33865005e-02, 1.67968712e-01,\n",
       "          1.14429744e-02, 2.84978308e-02, 2.52928489e-02, 1.38479154e-01,\n",
       "          1.42786735e-01, 8.22186183e-02],\n",
       "         [8.72902885e-02, 1.08757482e-01, 5.30151673e-02, 4.86520971e-03,\n",
       "          5.12796500e-02, 1.09546003e-01, 7.91238053e-02, 1.20436006e-01,\n",
       "          1.17104313e-02, 8.77020487e-02],\n",
       "         [9.91854275e-02, 2.83249014e-02, 6.69116481e-02, 6.71931520e-02,\n",
       "          1.49637539e-02, 1.28983415e-01, 9.55476912e-02, 3.66893467e-02,\n",
       "          8.54130321e-02, 1.70595575e-01],\n",
       "         [3.80677352e-03, 1.05801364e-01, 1.06909884e-01, 1.65913090e-02,\n",
       "          1.29545303e-02, 1.98873034e-02, 6.06362183e-02, 7.67668005e-02,\n",
       "          1.02941583e-02, 1.11983818e-01],\n",
       "         [5.16713119e-02, 4.97136897e-02, 3.27267985e-02, 7.08980897e-02,\n",
       "          3.77874901e-02, 1.73420845e-01, 1.16401171e-01, 3.23900352e-02,\n",
       "          5.11773479e-02, 7.03553929e-02],\n",
       "         [1.32519941e-01, 1.52130359e-01, 9.01257726e-02, 1.03433160e-01,\n",
       "          3.00780900e-02, 4.01751537e-02, 1.22832468e-01, 6.34001833e-02,\n",
       "          1.57111563e-01, 7.54904877e-02],\n",
       "         [1.46409614e-01, 6.47641907e-02, 1.46452782e-01, 9.60280450e-02,\n",
       "          7.82510569e-03, 1.67571455e-01, 1.05477472e-01, 1.28135087e-01,\n",
       "          7.14356189e-02, 9.46396515e-02],\n",
       "         [1.04124241e-01, 5.91222056e-02, 1.21650230e-01, 6.50987580e-02,\n",
       "          5.11834861e-02, 1.35418494e-02, 1.13684673e-01, 4.78063305e-02,\n",
       "          3.23489514e-02, 1.71470862e-01],\n",
       "         [7.05666554e-02, 1.16829548e-01, 1.00678244e-01, 1.37341209e-01,\n",
       "          1.64811361e-01, 1.42873810e-01, 9.93077034e-02, 5.71012571e-02,\n",
       "          7.65882411e-02, 4.05517211e-03],\n",
       "         [3.94360703e-02, 9.11619786e-02, 7.78508617e-02, 8.84757914e-03,\n",
       "          1.41916248e-01, 1.71535571e-01, 6.06139770e-02, 9.11881526e-02,\n",
       "          1.84506445e-02, 1.53674750e-01],\n",
       "         [1.12207209e-01, 1.50841840e-01, 1.19875288e-01, 1.59748873e-01,\n",
       "          1.41010724e-01, 1.30987107e-01, 1.21408164e-01, 2.03988480e-02,\n",
       "          4.33838188e-02, 1.70538539e-01],\n",
       "         [5.36341536e-02, 4.99306567e-02, 6.53811799e-02, 2.50797107e-03,\n",
       "          1.29760402e-01, 1.38914208e-01, 1.40480807e-01, 1.61488311e-01,\n",
       "          7.07816444e-02, 1.29997902e-01],\n",
       "         [1.28284047e-01, 1.04915064e-01, 2.35427497e-02, 1.49076140e-01,\n",
       "          5.58857288e-03, 1.51836806e-01, 1.06120148e-01, 1.38505119e-01,\n",
       "          1.23906774e-02, 1.67406255e-01],\n",
       "         [1.53855825e-01, 3.70157820e-02, 1.59474553e-01, 1.06924165e-01,\n",
       "          3.59069645e-02, 1.60836865e-01, 9.17251177e-02, 7.05035507e-02,\n",
       "          8.25950826e-02, 4.56448568e-02],\n",
       "         [6.65683622e-02, 1.12305715e-02, 1.41629093e-01, 1.25307034e-01,\n",
       "          3.83072265e-02, 6.18200458e-02, 1.44019225e-01, 1.06841459e-01,\n",
       "          6.50164540e-02, 7.48864501e-02],\n",
       "         [1.04439243e-03, 8.01838023e-02, 2.68692207e-03, 5.18827370e-02,\n",
       "          1.40737327e-01, 1.61798540e-02, 1.75619697e-01, 1.29013169e-01,\n",
       "          1.31811568e-01, 1.48262328e-01],\n",
       "         [1.12594314e-01, 8.66647316e-02, 5.00363838e-02, 1.20113301e-01,\n",
       "          1.02855410e-01, 2.27469377e-02, 7.79446720e-02, 1.42823524e-01,\n",
       "          1.43170204e-02, 6.25818512e-02],\n",
       "         [4.05395580e-03, 3.08341128e-02, 3.44328404e-02, 2.90292401e-02,\n",
       "          1.26375280e-01, 2.24426469e-03, 1.05626579e-01, 1.21699634e-01,\n",
       "          1.02631996e-02, 4.90335398e-02],\n",
       "         [6.22524968e-02, 1.57374527e-01, 6.07886215e-02, 1.47780462e-01,\n",
       "          5.83298480e-02, 1.20924824e-01, 6.65537555e-02, 1.19535069e-01,\n",
       "          1.01109624e-02, 1.19755141e-01],\n",
       "         [9.47364354e-02, 1.18723734e-01, 1.23209659e-01, 1.76533685e-01,\n",
       "          8.31297663e-02, 2.22923750e-02, 1.34317199e-01, 1.42550928e-01,\n",
       "          1.45205433e-02, 4.66362794e-02],\n",
       "         [3.16010688e-02, 8.77634268e-02, 1.61278575e-01, 1.10086349e-01,\n",
       "          2.98580807e-02, 6.51073111e-02, 6.04885167e-02, 1.26735906e-02,\n",
       "          4.14352987e-02, 1.98140791e-02],\n",
       "         [1.07585781e-01, 5.73186086e-04, 9.62426642e-02, 4.26961221e-02,\n",
       "          1.73776104e-01, 1.72034218e-01, 9.74571451e-02, 7.44988409e-02,\n",
       "          1.34267718e-01, 9.48659808e-02],\n",
       "         [1.13679118e-01, 1.19503989e-01, 9.77918649e-02, 2.08180996e-02,\n",
       "          8.83990857e-02, 1.69566348e-01, 1.02432294e-01, 1.59871021e-01,\n",
       "          8.05119835e-02, 2.22997392e-02]])],\n",
       " [array([0.65798543, 0.10125231]),\n",
       "  array([0.92347458, 0.07404215]),\n",
       "  array([0.15109654, 0.05463848, 0.39727985, 0.29439551, 0.30073058,\n",
       "         0.28589352, 0.2278403 , 0.08505245, 0.42196299, 0.34723329])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_nodes = 10\n",
    "dropout = 0.1\n",
    "lr = 0.01\n",
    "epoch = 10\n",
    "\n",
    "####BEGIN CODE HERE####\n",
    "bias_0 = bias_initialization(2)\n",
    "bias_1 = bias_initialization(2)\n",
    "bias_2 = bias_initialization(10)\n",
    "bias = [bias_0, bias_1, bias_2]\n",
    "\n",
    "weights_0 = []\n",
    "ratio_0 = 2\n",
    "for i in range(ratio_0):\n",
    "    t = kernel_initialization(4, 4)\n",
    "    weights_0.append(kernel_to_matrix(t, 28))\n",
    "\n",
    "weights_1 = []\n",
    "ratio_1 = 2\n",
    "for i in range(ratio_1):\n",
    "    t = kernel_initialization(4, 4)\n",
    "    weights_1.append(kernel_to_matrix(t, 12))\n",
    "\n",
    "weights_2 = He_initialize_weight(64, 10)\n",
    "weights = [weights_0, weights_1, weights_2]\n",
    "\n",
    "activations = ['max', 'max', 'relu', 'softmax']\n",
    "layers = ['conv', 'maxpool', \n",
    "          'conv', 'conv', 'residual', \n",
    "          'conv', 'conv', 'residual',\n",
    "         'fc', 'fc', 'fc']\n",
    "x, y = gen_dummy()\n",
    "\n",
    "train(x, y, weights, bias, activations, layers, epoch, lr, dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation explanation:\n",
    "\n",
    "What are you experimenting on? Any aspect of your network that is not learned by SGD, and is not the number of layers of the network, is free for experimentation. Learning rate, number of training samples, epochs, nodes per layer etc etc are a few examples.\n",
    "\n",
    "There are a lot of ways to get full credit on this experimentation portion, as there is no dedicated format I am requesting. There is no benchmark. There is no \"you must get 100% accuracy\". I simply ask you to see how well your models can do with the restrictions in place.\n",
    "\n",
    "Best of luck, \n",
    "Ryan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation notes:\n",
    "\n",
    "    This cell has been left in text format for you to freely edit and keep track of your experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
